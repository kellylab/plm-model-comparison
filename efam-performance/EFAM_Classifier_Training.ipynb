{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2qeCkSDjKs9g0KcTPbHUg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pikanaeri/plm-model-comparison/blob/main/efam-performance/EFAM_Classifier_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpcfxV5SNcaS"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras import backend as K\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "phrog_data_dir = 'final_embeddings' #ensure that this aligns with the model that you use\n",
        "phrog_metadata = pd.read_csv('PHROG_index.csv')\n",
        "\n",
        "### To train the Esm2 model, some of the empty embeddings had to be removed\n",
        "empty_phrogs = []\n",
        "os.chdir(\"final_embeddings\")\n",
        "\n",
        "for i in os.listdir():\n",
        "  if i.endswith(\".pkl\"):\n",
        "    f2 = open(i, \"rb\")\n",
        "    phrog_num = i.replace(\".pkl\", \"\")\n",
        "    ar = pickle.load(f2)\n",
        "    if len(ar) == 0:\n",
        "      empty_phrogs.append(phrog_num)\n",
        "    f2.close()\n",
        "\n",
        "print(len(empty_phrogs), \" empty\")\n",
        "for phrog in empty_phrogs:\n",
        "  phrog_metadata = phrog_metadata.drop(phrog_metadata[phrog_metadata[\"#phrog\"] == phrog].index)\n",
        "###\n",
        "\n",
        "sequence_number_per_family = 1000000\n",
        "phrog_metadata['Category'].value_counts()\n",
        "phrog_known = phrog_metadata[~phrog_metadata['Category'].isna()]\n",
        "phrog_known = phrog_known[~phrog_known['Category'].isin(['unknown function'])]\n",
        "len(phrog_known)\n",
        "cs = set(phrog_known['Category'])\n",
        "## dict for family:label -> {fl}\n",
        "## dict for family:vectors -> {fv}\n",
        "## dict for label:families -> {lf}\n",
        "fl = {}\n",
        "fv = {}\n",
        "lf = {}\n",
        "\n",
        "for c in cs:\n",
        "    ps = phrog_known[phrog_known['Category'] == c]['#phrog']\n",
        "    for p in ps:\n",
        "        fl[p] = c\n",
        "        try:\n",
        "            fv[p] = pickle.load(open('{0}/{1}.pkl' ''.format(phrog_data_dir, p), 'rb'))\n",
        "        except:\n",
        "            print('{0} embeddings not found' ''.format(p))\n",
        "            pass\n",
        "    lf[c] = list(set(ps).intersection(set(fv.keys())))\n",
        "\n",
        "from typing import List, Dict\n",
        "def subset_training_data(\n",
        "    vectors: Dict,\n",
        "    labels: Dict,\n",
        "    tr_families: List,\n",
        "    num_train_seq: int):\n",
        "    tr_vectors = [random.sample(list(vectors[f]), min(num_train_seq, len(vectors[f]))) for f in tr_families]\n",
        "    tr_vectors = np.vstack(tr_vectors)\n",
        "    tr_label = [[labels[f]] * min(num_train_seq, len(vectors[f])) for f in tr_families]\n",
        "    tr_label = [j for i in tr_label for j in i]\n",
        "    return tr_vectors, tr_label\n",
        "\n",
        "train_families = list(set(fv.keys()))\n",
        "train_x, train_y = subset_training_data(\n",
        "    vectors=fv,\n",
        "    labels=fl,\n",
        "    tr_families=train_families,\n",
        "    num_train_seq=sequence_number_per_family)\n",
        "np.unique(np.array(train_y), return_counts=True)\n",
        "# label binarize\n",
        "# convert the labels from integers to vectors\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(train_y)\n",
        "trainX = train_x\n",
        "# model architechture\n",
        "model = tf.keras.Sequential([keras.layers.Dense(512, input_shape=(1024,), activation=\"relu\"), #change size of the top layer to match the embedding dimensions\n",
        "                             keras.layers.Dropout(0.2),\n",
        "                             keras.layers.Dense(256, input_shape=(512,), activation=\"relu\"),\n",
        "                             keras.layers.Dropout(0.2),\n",
        "                             keras.layers.Dense(128, input_shape=(256,), activation=\"relu\"),\n",
        "                             keras.layers.Dense(9, activation=\"softmax\")])\n",
        "\n",
        "n_epoch = 5\n",
        "opt = Adam(0.0001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "H = model.fit(trainX, trainY, epochs=n_epoch, batch_size=60)\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, n_epoch), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, n_epoch), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "phrog_unknown = phrog_metadata[~phrog_metadata['Category'].isna()]\n",
        "phrog_unknown = phrog_unknown[phrog_unknown['Category'].isin(['unknown function'])]\n",
        "ufv = {}\n",
        "for p in phrog_unknown['#phrog']:\n",
        "    try:\n",
        "        ufv[p] = pickle.load(open('{0}/{1}.pkl' ''.format(phrog_data_dir,p), 'rb'))\n",
        "    except:\n",
        "        print('{0} embeddings not found' ''.format(p))\n",
        "        pass\n",
        "\n",
        "confidence = 0.8\n",
        "confident_unknown = []\n",
        "unconfident_unknown = []\n",
        "for f in tqdm(ufv.keys()):\n",
        "  try:\n",
        "    pred_f = model.predict(ufv[f], verbose=0)\n",
        "    pred_f = np.mean(pred_f, axis=0)\n",
        "    if sum(pred_f > confidence) > 0:\n",
        "        confident_unknown.append(f)\n",
        "    else:\n",
        "        unconfident_unknown.append(f)\n",
        "  except:\n",
        "    print('empty embedding ', f)\n",
        "    pass\n",
        "\n",
        "len(unconfident_unknown)\n",
        "ufv_vectors = [random.sample(list(ufv[f]), min(sequence_number_per_family, len(ufv[f]))) for f in unconfident_unknown]\n",
        "ufv_vectors = np.vstack(ufv_vectors)\n",
        "ufv_label = ['unknown'] * len(ufv_vectors)\n",
        "len(ufv_vectors)\n",
        "\n",
        "vectors = np.concatenate((train_x, ufv_vectors))\n",
        "label = np.concatenate((train_y, ufv_label))\n",
        "np.unique(np.array(label), return_counts=True)\n",
        "trainX = vectors\n",
        "trainY = label\n",
        "# label binarize\n",
        "# convert the labels from integers to vectors\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "# model architechture\n",
        "model2 = tf.keras.Sequential([keras.layers.Dense(512, input_shape=(1024,), activation=\"relu\"), #change size of the top layer to match the embedding dimensions\n",
        "                              keras.layers.Dropout(0.2),\n",
        "                              keras.layers.Dense(256, input_shape=(512,), activation=\"relu\"),\n",
        "                              keras.layers.Dropout(0.2),\n",
        "                              keras.layers.Dense(128, input_shape=(256,), activation=\"relu\"),\n",
        "                              keras.layers.Dense(10, activation=\"softmax\")])\n",
        "n_epoch = 5\n",
        "opt = Adam(0.0001)\n",
        "model2.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "H2 = model2.fit(trainX, trainY, epochs=n_epoch, batch_size=60)\n",
        "# plot the training loss and accuracy\n",
        "plt.rcParams[\"figure.figsize\"]=8,8\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, n_epoch), H2.history[\"loss\"], label=\"train_loss\")\n",
        "\n",
        "plt.plot(np.arange(0, n_epoch), H2.history[\"accuracy\"], label=\"train_acc\")\n",
        "\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "os.mkdir('models')\n",
        "model2.save('models/model_unknown.keras')\n",
        "pickle.dump(lb, open('models/model_unknown_lb.pkl', 'wb'))\n",
        "\n"
      ]
    }
  ]
}